{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e6f54c",
   "metadata": {},
   "source": [
    "### Problem 3. 주어진  사진  샘플  8장에  원통형  투영을  적용해  하나의 이미지로 합치기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba19ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "img1 = cv2.imread('./image/problem_3/1.jpg')\n",
    "img2 = cv2.imread('./image/problem_3/2.jpg')\n",
    "img3 = cv2.imread('./image/problem_3/3.jpg')\n",
    "img4 = cv2.imread('./image/problem_3/4.jpg')\n",
    "img5 = cv2.imread('./image/problem_3/5.jpg')\n",
    "img6 = cv2.imread('./image/problem_3/6.jpg')\n",
    "img7 = cv2.imread('./image/problem_3/7.jpg')\n",
    "img8 = cv2.imread('./image/problem_3/8.jpg')\n",
    "\n",
    "\n",
    "img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
    "img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
    "img3 = cv2.cvtColor(img3, cv2.COLOR_BGR2RGB)\n",
    "img4 = cv2.cvtColor(img4, cv2.COLOR_BGR2RGB)\n",
    "img5 = cv2.cvtColor(img5, cv2.COLOR_BGR2RGB)\n",
    "img6 = cv2.cvtColor(img6, cv2.COLOR_BGR2RGB)\n",
    "img7 = cv2.cvtColor(img7, cv2.COLOR_BGR2RGB)\n",
    "img8 = cv2.cvtColor(img8, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "578286c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import numpy as np\n",
    "\n",
    "def get_camera_matrices(image_path, sensor_width=23.5, sensor_height=15.6):\n",
    "    img = Image.open(image_path)\n",
    "    exif_data = img._getexif()\n",
    "\n",
    "    if exif_data is None:\n",
    "        print(\"No EXIF data found.\")\n",
    "        return None, None\n",
    "    \n",
    "    focal_length = None\n",
    "    focal_length_35mm = None\n",
    "    image_width, image_height = img.size\n",
    "\n",
    "    for tag, value in exif_data.items():\n",
    "        tag_name = TAGS.get(tag, tag)\n",
    "        \n",
    "        # FocalLength (기본)\n",
    "        if tag_name == 'FocalLength':\n",
    "            if isinstance(value, tuple):\n",
    "                focal_length = value[0] / value[1]\n",
    "            else:\n",
    "                focal_length = float(value)\n",
    "\n",
    "        # FocalLengthIn35mmFilm (35mm 환산)\n",
    "        if tag_name == 'FocalLengthIn35mmFilm':\n",
    "            focal_length_35mm = int(value)\n",
    "\n",
    "    # 기본 Focal Length가 있으면 사용하고, 없으면 35mm 환산값 사용\n",
    "    if focal_length_35mm is not None:\n",
    "        # 35mm 환산을 기준으로 초점 거리 계산\n",
    "        focal_length = (focal_length_35mm / 35.0) * sensor_width\n",
    "    \n",
    "    # 기본 Focal Length도 없는 경우\n",
    "    if focal_length is None:\n",
    "        print(\"No valid focal length found in EXIF data.\")\n",
    "        return None, None\n",
    "\n",
    "    # K 계산 (float32 타입으로 설정)\n",
    "    fx = (focal_length / sensor_width) * image_width\n",
    "    fy = (focal_length / sensor_height) * image_height  # 수직 초점 거리 계산\n",
    "    cx = image_width / 2\n",
    "    cy = image_height / 2\n",
    "\n",
    "    K = np.array([\n",
    "        [fx, 0, cx],\n",
    "        [0, fy, cy],\n",
    "        [0, 0, 1]\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    # 기본적으로 R은 단위 행렬로 시작\n",
    "    R = np.eye(3, dtype=np.float32)\n",
    "\n",
    "    return K, R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dacbd166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 1:\n",
      "  ResolutionUnit: 2\n",
      "  ExifOffset: 358\n",
      "  ImageDescription:                                \n",
      "  Make: SONY\n",
      "  Model: NEX-5N\n",
      "  Software: GIMP 2.8.3\n",
      "  Orientation: 1\n",
      "  DateTime: 2014:04:09 18:33:06\n",
      "  YCbCrPositioning: 2\n",
      "  XResolution: 350.0\n",
      "  YResolution: 350.0\n",
      "  ExifVersion: b'0230'\n",
      "  ComponentsConfiguration: b'\\x01\\x02\\x03\\x00'\n",
      "  CompressedBitsPerPixel: 2.0\n",
      "  DateTimeOriginal: 2013:01:22 11:50:44\n",
      "  DateTimeDigitized: 2013:01:22 11:50:44\n",
      "  BrightnessValue: 8.5171875\n",
      "  ExposureBiasValue: 0.0\n",
      "  MaxApertureValue: 3.6171875\n",
      "  MeteringMode: 5\n",
      "  LightSource: 0\n",
      "  Flash: 16\n",
      "  FocalLength: 18.0\n",
      "  ColorSpace: 1\n",
      "  ExifImageWidth: 2000\n",
      "  SceneCaptureType: 0\n",
      "  ExifImageHeight: 1329\n",
      "  Contrast: 0\n",
      "  Saturation: 0\n",
      "  Sharpness: 0\n",
      "  FileSource: b'\\x03'\n",
      "  ExposureTime: 0.005\n",
      "  ExifInteroperabilityOffset: 39892\n",
      "  FNumber: 7.1\n",
      "  SceneType: b'\\x01'\n",
      "  ExposureProgram: 2\n",
      "  CustomRendered: 0\n",
      "  ISOSpeedRatings: 100\n",
      "  ExposureMode: 0\n",
      "  FlashPixVersion: b'0100'\n",
      "  WhiteBalance: 0\n",
      "  FocalLengthIn35mmFilm: 27\n",
      "Image 2:\n",
      "  ResolutionUnit: 2\n",
      "  ExifOffset: 358\n",
      "  ImageDescription:                                \n",
      "  Make: SONY\n",
      "  Model: NEX-5N\n",
      "  Software: GIMP 2.8.3\n",
      "  Orientation: 1\n",
      "  DateTime: 2014:04:09 18:32:37\n",
      "  YCbCrPositioning: 2\n",
      "  XResolution: 350.0\n",
      "  YResolution: 350.0\n",
      "  ExifVersion: b'0230'\n",
      "  ComponentsConfiguration: b'\\x01\\x02\\x03\\x00'\n",
      "  CompressedBitsPerPixel: 2.0\n",
      "  DateTimeOriginal: 2013:01:22 11:50:36\n",
      "  DateTimeDigitized: 2013:01:22 11:50:36\n",
      "  BrightnessValue: 9.1734375\n",
      "  ExposureBiasValue: 0.0\n",
      "  MaxApertureValue: 3.6171875\n",
      "  MeteringMode: 5\n",
      "  LightSource: 0\n",
      "  Flash: 16\n",
      "  FocalLength: 18.0\n",
      "  ColorSpace: 1\n",
      "  ExifImageWidth: 2000\n",
      "  SceneCaptureType: 0\n",
      "  ExifImageHeight: 1329\n",
      "  Contrast: 0\n",
      "  Saturation: 0\n",
      "  Sharpness: 0\n",
      "  FileSource: b'\\x03'\n",
      "  ExposureTime: 0.01\n",
      "  ExifInteroperabilityOffset: 39892\n",
      "  FNumber: 10.0\n",
      "  SceneType: b'\\x01'\n",
      "  ExposureProgram: 2\n",
      "  CustomRendered: 0\n",
      "  ISOSpeedRatings: 100\n",
      "  ExposureMode: 0\n",
      "  FlashPixVersion: b'0100'\n",
      "  WhiteBalance: 0\n",
      "  FocalLengthIn35mmFilm: 27\n",
      "Image 3:\n",
      "  ResolutionUnit: 2\n",
      "  ExifOffset: 358\n",
      "  ImageDescription:                                \n",
      "  Make: SONY\n",
      "  Model: NEX-5N\n",
      "  Software: GIMP 2.8.3\n",
      "  Orientation: 1\n",
      "  DateTime: 2014:04:09 18:32:51\n",
      "  YCbCrPositioning: 2\n",
      "  XResolution: 350.0\n",
      "  YResolution: 350.0\n",
      "  ExifVersion: b'0230'\n",
      "  ComponentsConfiguration: b'\\x01\\x02\\x03\\x00'\n",
      "  CompressedBitsPerPixel: 2.0\n",
      "  DateTimeOriginal: 2013:01:22 11:50:40\n",
      "  DateTimeDigitized: 2013:01:22 11:50:40\n",
      "  BrightnessValue: 8.9078125\n",
      "  ExposureBiasValue: 0.0\n",
      "  MaxApertureValue: 3.6171875\n",
      "  MeteringMode: 5\n",
      "  LightSource: 0\n",
      "  Flash: 16\n",
      "  FocalLength: 18.0\n",
      "  ColorSpace: 1\n",
      "  ExifImageWidth: 2000\n",
      "  SceneCaptureType: 0\n",
      "  ExifImageHeight: 1329\n",
      "  Contrast: 0\n",
      "  Saturation: 0\n",
      "  Sharpness: 0\n",
      "  FileSource: b'\\x03'\n",
      "  ExposureTime: 0.005\n",
      "  ExifInteroperabilityOffset: 39892\n",
      "  FNumber: 7.1\n",
      "  SceneType: b'\\x01'\n",
      "  ExposureProgram: 2\n",
      "  CustomRendered: 0\n",
      "  ISOSpeedRatings: 100\n",
      "  ExposureMode: 0\n",
      "  FlashPixVersion: b'0100'\n",
      "  WhiteBalance: 0\n",
      "  FocalLengthIn35mmFilm: 27\n"
     ]
    }
   ],
   "source": [
    "# 각 이미지의 주요 메타데이터(촬영 시간, 모델명 등) 비교\n",
    "def get_metadata(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    exif_data = img._getexif()\n",
    "    \n",
    "    if exif_data is None:\n",
    "        print(\"No EXIF data found.\")\n",
    "        return None\n",
    "    \n",
    "    metadata = {}\n",
    "    for tag, value in exif_data.items():\n",
    "        tag_name = TAGS.get(tag, tag)\n",
    "        metadata[tag_name] = value\n",
    "\n",
    "    return metadata\n",
    "def print_metadata_summary(meta, idx):\n",
    "    print(f\"Image {idx}:\")\n",
    "    for key, value in meta.items():\n",
    "        if key not in ['PrintImageMatching','UserComment','MakerNote','GPSInfo']:\n",
    "\n",
    "           print(f\"  {key}: {value}\")\n",
    "print_metadata_summary(get_metadata('./image/problem_2/3.jpg'), 1)\n",
    "print_metadata_summary(get_metadata('./image/problem_2/4.jpg'), 2)\n",
    "print_metadata_summary(get_metadata('./image/problem_2/5.jpg'), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d211f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import itertools\n",
    "# 2. 키포인트 검출 및 매칭\n",
    "def detect_keypoints(image):\n",
    "    sift = cv2.SIFT_create()\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if image.ndim == 3 else image\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "# 3. 매칭된 키포인트 쌍 및 score 계산\n",
    "from typing import Tuple\n",
    "def compute_matching(kp1, des1, kp2, des2, ratio=0.7, max_distance=80.0, use_ransac=True):\n",
    "    # 특징점이 없을 때\n",
    "    if des1 is None or des2 is None:\n",
    "        return 0, [], np.eye(3)\n",
    "    \n",
    "    # 특징점 매칭\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(des1, des2, k=2)\n",
    "\n",
    "    # Lowe's ratio test\n",
    "    good_matches = [m[0] for m in matches if len(m) == 2 and m[0].distance < ratio * m[1].distance]\n",
    "\n",
    "    # 거리 기반 필터링\n",
    "    good_matches = [m for m in good_matches if m.distance < max_distance]\n",
    "    \n",
    "    # 호모그래피 검증\n",
    "    if use_ransac and len(good_matches) >= 4:\n",
    "        src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "        H, mask = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
    "        good_matches = [m for i, m in enumerate(good_matches) if mask[i]]\n",
    "        score = len(good_matches)\n",
    "        return score, good_matches, H\n",
    "    else:\n",
    "        raise ValueError(\"Not enough matches found to compute homography.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d65d2043",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchData:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        MatchData 객체 초기화\n",
    "        데이터는 각각 순서대로 pair, kp1, kp2, score, good_matches, H로 구성됨\n",
    "        pair: (img1, img2) 이미지 index 쌍\n",
    "        kp1, kp2: 각각의 이미지에서 검출된 키포인트\n",
    "        score: 매칭된 키포인트 쌍의 수\n",
    "        good_matches: 매칭된 키포인트 쌍\n",
    "        H: 호모그래피 행렬\n",
    "        '''\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, pair,kp1,kp2, score, good_matches, H):\n",
    "        if not isinstance(pair, tuple) or len(pair) != 2:\n",
    "            raise ValueError(\"Pair must be a tuple of (img1, img2).\")\n",
    "        if type(pair[0]) != int or type(pair[1]) != int:\n",
    "            raise ValueError(\"Pair elements must be integers.\")\n",
    "        if not isinstance(kp1, tuple) or not isinstance(kp2, tuple):\n",
    "            raise ValueError(\"Keypoints must be lists.\")\n",
    "        if not isinstance(score, int):\n",
    "            raise ValueError(\"Score must be an integer.\")\n",
    "        if not isinstance(good_matches, list):\n",
    "            raise ValueError(\"Good matches must be a list.\")\n",
    "        if not isinstance(H, np.ndarray) or H.shape != (3, 3):\n",
    "            raise ValueError(\"H must be a 3x3 numpy array.\")\n",
    "        if score < 0:\n",
    "            raise ValueError(\"Score must be non-negative.\")\n",
    "        if len(good_matches) < 4:\n",
    "            raise ValueError(\"At least 4 good matches are required to compute homography.\")\n",
    "        self.data.append((pair,kp1,kp2, score, good_matches, H))\n",
    "\n",
    "    def __setitem__(self, idx, value):\n",
    "        if not isinstance(value, tuple) or len(value) !=6:\n",
    "            raise ValueError(\"Value must be a tuple of (pair, kp1, kp2, score, good_matches, H).\")\n",
    "\n",
    "    def sort(self, reverse=False):\n",
    "        self.data.sort(key=lambda x: x[3], reverse=reverse) # sort by score\n",
    "    \n",
    "    def index(self, score):\n",
    "        \"\"\"\n",
    "        Return the index of the first matching item with the given score.\n",
    "        \"\"\"\n",
    "        for idx, item in enumerate(self.data):\n",
    "            if item[3] == score:\n",
    "                return idx\n",
    "            if type(score) not in (int, float):\n",
    "                raise TypeError(\"Score must be an integer or float.\")\n",
    "        raise ValueError(f\"Score {score} not found in MatchData.\") \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self.data):\n",
    "            raise IndexError(\"Index out of range.\")\n",
    "        return self.data[idx]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c2beba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Blender:\n",
    "    def __init__(self, blending_method=\"none\"):\n",
    "        self.blending_method = blending_method.lower()\n",
    "        self.H_matrices = None\n",
    "        \n",
    "        # 허용된 블렌딩 방법 체크\n",
    "        if self.blending_method not in [\"none\", \"simple\"]:\n",
    "            raise ValueError(\"잘못된 블렌딩 방법입니다. 'none' 또는 'simple' 중 하나를 선택하세요.\")\n",
    "\n",
    "    def set_h_matrices(self, H_matrices):\n",
    "        \"\"\"\n",
    "        H_matrices를 설정하는 메서드\n",
    "        \"\"\"\n",
    "        if not isinstance(H_matrices, dict):\n",
    "            raise ValueError(\"H_matrices는 딕셔너리여야 합니다.\")\n",
    "        self.H_matrices = H_matrices\n",
    "\n",
    "    def blend_images(self, images):\n",
    "        \"\"\"\n",
    "        선택된 블렌딩 방법에 따라 이미지를 블렌딩\n",
    "        \n",
    "        Parameters:\n",
    "        - images (list): 블렌딩할 이미지 리스트\n",
    "        \n",
    "        Returns:\n",
    "        - np.ndarray: 블렌딩된 최종 이미지\n",
    "        \"\"\"\n",
    "        if self.H_matrices is None:\n",
    "            raise ValueError(\"H_matrices가 설정되지 않았습니다. 먼저 set_h_matrices를 호출하세요.\")\n",
    "        \n",
    "        if self.blending_method == \"none\":\n",
    "            return self.none_blend(images)\n",
    "        elif self.blending_method == \"simple\":\n",
    "            return self.simple_blend(images)\n",
    "        else:\n",
    "            raise ValueError(\"지원되지 않는 블렌딩 방법입니다.\")\n",
    "        \n",
    "    def none_blend(self, images):\n",
    "        \"\"\"\n",
    "        블렌딩 없이 단순히 첫 번째 이미지를 정렬하여 반환\n",
    "        \"\"\"\n",
    "        # 첫 번째 앵커 인덱스 설정 (단위 행렬이 할당된 인덱스)\n",
    "        anchor_index = list(self.H_matrices.keys())[0]\n",
    "        anchor_img = images[anchor_index]\n",
    "        H_anchor = self.H_matrices[anchor_index]\n",
    "\n",
    "        # 출력 캔버스 크기 계산\n",
    "        corners = []\n",
    "        for idx, H in self.H_matrices.items():\n",
    "            h, w = images[idx].shape[:2]\n",
    "            corners += [H @ np.array([0, 0, 1]),\n",
    "                        H @ np.array([w, 0, 1]),\n",
    "                        H @ np.array([0, h, 1]),\n",
    "                        H @ np.array([w, h, 1])]\n",
    "        \n",
    "        # 코너 좌표 정규화\n",
    "        corners = np.array(corners)\n",
    "        corners /= corners[:, 2].reshape(-1, 1)\n",
    "        min_x, min_y = np.min(corners[:, :2], axis=0).astype(int)\n",
    "        max_x, max_y = np.max(corners[:, :2], axis=0).astype(int)\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "\n",
    "        # 오프셋 변환\n",
    "        translation = np.array([[1, 0, -min_x], [0, 1, -min_y], [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        # 앵커 이미지 왜곡 (단위 행렬 적용)\n",
    "        stitched_image = cv2.warpPerspective(anchor_img, translation @ H_anchor, (width, height))\n",
    "\n",
    "        # 다른 이미지 추가 (단순 블렌딩)\n",
    "        for idx, H in self.H_matrices.items():\n",
    "            if idx == anchor_index:\n",
    "                continue\n",
    "            warped_img = cv2.warpPerspective(images[idx], translation @ H, (width, height))\n",
    "            mask = (warped_img.sum(axis=2) > 0).astype(np.uint8)\n",
    "            stitched_image[mask > 0] = warped_img[mask > 0]\n",
    "\n",
    "        return stitched_image\n",
    "    \n",
    "    def simple_blend(self, images):\n",
    "        \"\"\"\n",
    "        간단한 평균 블렌딩\n",
    "        \"\"\"\n",
    "        # 출력 캔버스 크기 계산\n",
    "        corners = []\n",
    "        for idx, H in self.H_matrices.items():\n",
    "            h, w = images[idx].shape[:2]\n",
    "            corners += [H @ np.array([0, 0, 1]),\n",
    "                        H @ np.array([w, 0, 1]),\n",
    "                        H @ np.array([0, h, 1]),\n",
    "                        H @ np.array([w, h, 1])]\n",
    "        \n",
    "        corners = np.array(corners)\n",
    "        corners /= corners[:, 2].reshape(-1, 1)\n",
    "        min_x, min_y = np.min(corners[:, :2], axis=0).astype(int)\n",
    "        max_x, max_y = np.max(corners[:, :2], axis=0).astype(int)\n",
    "        width = max_x - min_x\n",
    "        height = max_y - min_y\n",
    "        \n",
    "        # 오프셋 변환 행렬\n",
    "        translation = np.array([[1, 0, -min_x], [0, 1, -min_y], [0, 0, 1]], dtype=np.float32)\n",
    "\n",
    "        # 블렌딩할 최종 이미지 초기화\n",
    "        blended_image = np.zeros((height, width, 3), dtype=np.float32)\n",
    "        blend_count = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "        # 모든 이미지의 픽셀 값 합산\n",
    "        for idx, H in self.H_matrices.items():\n",
    "            warped_img = cv2.warpPerspective(images[idx], translation @ H, (width, height))\n",
    "            mask = (warped_img.sum(axis=2) > 0).astype(np.float32)\n",
    "            blended_image += warped_img * mask[..., np.newaxis]\n",
    "            blend_count += mask\n",
    "        \n",
    "        # 정규화하여 최종 블렌딩 이미지 생성\n",
    "        blended_image /= np.maximum(blend_count[..., np.newaxis], 1)\n",
    "\n",
    "        return blended_image.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25b7ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageStitcher(Blender):\n",
    "    def __init__(self, blending_method=\"none\"):\n",
    "        Blender.__init__(self, blending_method)\n",
    "        self.H_matrices = None\n",
    "\n",
    "    def __call__(self, match_data, images):\n",
    "        # 매칭 데이터가 비어 있는지 확인\n",
    "        if len(match_data) == 0:\n",
    "            raise ValueError(\"매칭 데이터가 비어 있습니다.\")\n",
    "        \n",
    "        # 매칭 데이터 정렬 (내림차순)\n",
    "        match_data.sort(reverse=True)\n",
    "        \n",
    "        # 앵커 이미지와 그 인덱스 찾기\n",
    "        best_index = self.find_anchor(match_data)\n",
    "        \n",
    "        # 호모그래피 그래프 생성\n",
    "        self.H_matrices = self.build_homography(best_index, match_data, images)\n",
    "        \n",
    "        # 블렌딩 수행\n",
    "        return self.blend_images(images)\n",
    "    \n",
    "    def find_anchor(self, match_data):\n",
    "        # 가장 많은 매칭을 가진 이미지와 해당 인덱스 찾기\n",
    "        best_score = max(match_data.data, key=lambda x: x[3])[3]\n",
    "        best_index = match_data.index(best_score)\n",
    "        return best_index\n",
    "    \n",
    "    def build_homography(self, best_index, match_data, images):\n",
    "        \"\"\"\n",
    "        호모그래피 그래프 생성\n",
    "        \n",
    "        Parameters:\n",
    "        - best_index (int): 앵커 이미지의 인덱스\n",
    "        - match_data (MatchData): 이미지 쌍과 호모그래피를 포함한 매칭 데이터\n",
    "        - images (list): 블렌딩할 이미지 리스트\n",
    "        \n",
    "        Returns:\n",
    "        - dict: 각 이미지 인덱스와 그에 대한 호모그래피 행렬을 담은 딕셔너리\n",
    "        \"\"\"\n",
    "        # 모든 이미지 인덱스를 키로 사용하는 딕셔너리 초기화\n",
    "        H_matrices = {i: None for i in range(len(images))}\n",
    "        H_matrices[best_index] = np.eye(3, dtype=np.float32)  # 앵커 이미지는 단위 행렬\n",
    "\n",
    "        # 호모그래피 그래프 생성\n",
    "        for (pair, kp1, kp2, score, good_matches, H) in match_data:\n",
    "            # 이미지 인덱스로 변환 (tuple에서 직접 참조)\n",
    "            img1_idx, img2_idx = pair\n",
    "            \n",
    "            # 연결되지 않은 이미지를 연결\n",
    "            if H_matrices[img1_idx] is not None and H_matrices[img2_idx] is None:\n",
    "                H_matrices[img2_idx] = H_matrices[img1_idx] @ H\n",
    "            elif H_matrices[img2_idx] is not None and H_matrices[img1_idx] is None:\n",
    "                H_matrices[img1_idx] = H_matrices[img2_idx] @ np.linalg.inv(H)\n",
    "\n",
    "        return H_matrices\n",
    "\n",
    "    def each_transform(self, images):\n",
    "        \"\"\"\n",
    "        각 이미지에 대해 호모그래피 변환을 적용하여 정렬된 이미지를 반환\n",
    "        \n",
    "        Parameters:\n",
    "        - images (list): 블렌딩할 이미지 리스트\n",
    "        - H_matrices (dict): 각 이미지의 호모그래피 행렬\n",
    "        \n",
    "        Returns:\n",
    "        - list: 변환된 이미지 리스트\n",
    "        \"\"\"\n",
    "        transformed_images = []\n",
    "        for idx, H in self.H_matrices.items():\n",
    "            if H is not None:\n",
    "                transformed_img = cv2.warpPerspective(images[idx], H, (images[idx].shape[1], images[idx].shape[0]))\n",
    "                transformed_images.append(transformed_img)\n",
    "            else:\n",
    "                transformed_images.append(images[idx])\n",
    "        return transformed_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d14b22",
   "metadata": {},
   "source": [
    "### 실행 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd934d43",
   "metadata": {},
   "source": [
    "1. Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c2782c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n",
      "No EXIF data found.\n"
     ]
    }
   ],
   "source": [
    "# 카메라 매트릭스 및 회전 행렬 가져오기\n",
    "Ks, Rs = [], []\n",
    "for i in range(1, 9):\n",
    "    K, R = get_camera_matrices(f'./image/problem_3/{i}.jpg')\n",
    "    Ks.append(K)\n",
    "    Rs.append(R)\n",
    "\n",
    "image_list = [img1, img2, img3, img4, img5, img6, img7, img8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50b6d589",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m     img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 스케일 계산 (focal length / sensor width)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m scale = \u001b[43mK\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Warper 생성\u001b[39;00m\n\u001b[32m     11\u001b[39m warper = cv2.PyRotationWarper(\u001b[33m\"\u001b[39m\u001b[33mcylindrical\u001b[39m\u001b[33m\"\u001b[39m, scale)\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "warped_images = []\n",
    "# 각 이미지에 대해 호모그래피 변환 적용\n",
    "for i, (img, K, R) in enumerate(zip(image_list, Ks, Rs)):\n",
    "    # 채널 체크 (회색조 -> 컬러 변환)\n",
    "    if len(img.shape) == 2:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # 스케일 계산 (focal length / sensor width)\n",
    "    scale = K[0, 0]\n",
    "    # Warper 생성\n",
    "    warper = cv2.PyRotationWarper(\"cylindrical\", scale)\n",
    "    \n",
    "    # 워핑 적용\n",
    "    corner, warped_image = warper.warp(img, K.astype(np.float32), R.astype(np.float32), cv2.INTER_LINEAR, cv2.BORDER_CONSTANT)\n",
    "    \n",
    "    # 워핑된 이미지 저장\n",
    "    warped_images.append(warped_image)\n",
    "# Plot original and projected images\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 20))\n",
    "for i, (img, projected_img) in enumerate(zip(image_list, warped_images)):\n",
    "    # 원본 이미지\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f\"Original Image {i+1}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # 왜곡된 이미지\n",
    "    axes[i, 1].imshow(projected_img)\n",
    "    axes[i, 1].set_title(f\"Warped Image {i+1}\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 워핑된 이미지로 업데이트\n",
    "image_list = warped_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e644078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. SIFT 특징점 검출  시각화 \n",
    "image_list = warped_images.copy()\n",
    "kps, dess = [], []\n",
    "for i, img in enumerate(image_list):\n",
    "    sift = cv2.SIFT_create()\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if img.ndim == 3 else img\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    kps.append(kp)\n",
    "    dess.append(des)\n",
    "\n",
    "# Draw keypoints on the images\n",
    "keypoint_colcor = []\n",
    "for i, (img, kp) in enumerate(zip(image_list, kps)):\n",
    "    img_kp = cv2.drawKeypoints(img, kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    keypoint_colcor.append(img_kp)\n",
    "    \n",
    "fig, axes = plt.subplots(3, 2, figsize=(20,30))\n",
    "\n",
    "for i, (img, img_kp) in enumerate(zip(image_list, keypoint_colcor)):\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(f\"Original Image {i+1}\")\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    axes[i, 1].imshow(img_kp)\n",
    "    axes[i, 1].set_title(f\"Keypoints Image {i+1}\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640d85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_data = MatchData()\n",
    "# 3. 매칭된 키포인트 쌍 및 score 계산\n",
    "from itertools import combinations\n",
    "id_list = [i for i in range(len(image_list))]\n",
    "kps_pairs = combinations(zip(id_list, kps, dess), 2)\n",
    "for (idx1, kp1, des1), (idx2, kp2, des2) in kps_pairs:\n",
    "    score, good_matches, H = compute_matching(kp1, des1, kp2, des2)\n",
    "    match_data.add((idx1, idx2), kp1, kp2, score, good_matches, H)\n",
    "# 4. 매칭된 키포인트 시각화\n",
    "def draw_matches(img1, img2, kp1, kp2, good_matches):\n",
    "    # 매칭된 키포인트 시각화\n",
    "    img_matches = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    return img_matches\n",
    "for (pair, kp1, kp2, score, good_matches, H) in match_data.data:\n",
    "    img1_idx, img2_idx = pair\n",
    "    img1 = image_list[img1_idx]\n",
    "    img2 = image_list[img2_idx]\n",
    "    img_matches = draw_matches(img1, img2, kp1, kp2, good_matches)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img_matches)\n",
    "    plt.title(f'Matched Keypoints between Image {img1_idx+1, img2_idx+1} - Score: {score}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daae6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagesticher = ImageStitcher(blending_method=\"none\")\n",
    "# 5. 블렌딩 및 투영\n",
    "stitched_image = imagesticher(match_data, image_list)\n",
    "# 6. 변형된 이미지 시각화\n",
    "transformed_images = imagesticher.each_transform(image_list)\n",
    "for i, img in enumerate(transformed_images):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f'Transformed Image {i+1}')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 최종 이미지 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(stitched_image)\n",
    "plt.title('Stitched Image')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
